import streamlit as st
import asyncio
import pandas as pd
from krauler.core.crawler import start_crawl
from krauler.utils.storage import load_results_as_df

st.set_page_config(page_title="ğŸ•·ï¸ Krauler", layout="wide")
st.title("ğŸ•·ï¸ Krauler: AI-Ready Web Crawler")

with st.form("crawl_form"):
    seed_url = st.text_input("ğŸ”— Seed URL", placeholder="https://example.com")
    keyword_filter = st.text_input("ğŸ§  Keyword Filter (optional)")
    crawl_depth = st.slider("ğŸ“ Max Crawl Depth", 1, 5, 2)
    submit = st.form_submit_button("ğŸš€ Start Crawl")

if submit and seed_url:
    st.info(f"Starting crawl at: {seed_url}...")
    with st.spinner("Crawling in progress..."):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(
            start_crawl(seed_url=seed_url, max_depth=crawl_depth, keyword=keyword_filter or None)
        )
    st.success("âœ… Crawl completed!")
    df = load_results_as_df()
    if not df.empty:
        st.subheader("ğŸ“Š Crawl Results Preview")
        st.dataframe(df)
        st.download_button("â¬‡ï¸ Download JSON", df.to_json(orient="records"), file_name="crawl_results.json")
        st.download_button("â¬‡ï¸ Download CSV", df.to_csv(index=False), file_name="crawl_results.csv")
    else:
        st.warning("No crawl results found.")
